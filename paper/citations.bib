@article{abdiNeuralNetworkPrimer1994,
  title = {A {{Neural Network Primer}}},
  author = {Abdi, H.},
  date = {1994-09},
  journaltitle = {Journal of Biological Systems},
  shortjournal = {J. Biol. Syst.},
  volume = {02},
  number = {03},
  pages = {247--281},
  issn = {0218-3390, 1793-6470},
  doi = {10.1142/S0218339094000179},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0218339094000179},
  urldate = {2024-03-31},
  abstract = {Neural networks are composed of basic units somewhat analogous to neurons. These units are linked to each other by connections whose strength is modifiable as a result of a learning process or algorithm. Each of these units integrates independently (in paral lel) the information provided by its synapses in order to evaluate its state of activation. The unit response is then a linear or nonlinear function of its activation. Linear algebra concepts are used, in general, to analyze linear units, with eigenvectors and eigenvalues being the core concepts involved. This analysis makes clear the strong similarity between linear neural networks and the general linear model developed by statisticians. The linear models presented here are the perceptron and the linear associator. The behavior of nonlinear networks can be described within the framework of optimization and approximation techniques with dynamical systems (e.g., like those used to model spin glasses). One of the main notions used with nonlinear unit networks is the notion of attractor. When the task of the network is to associate a response with some specific input patterns, the most popular nonlinear technique consists of using hidden layers of neurons trained with back-propagation of error. The nonlinear models presented are the Hopfield network, the Boltzmann machine, the back-propagation network and the radial basis function network.},
  langid = {english}
}

@online{benderskySoftmaxFunctionIts,
  title = {The {{Softmax}} Function and Its Derivative - {{Eli Bendersky}}'s Website},
  author = {Bendersky, Eli},
  url = {https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/},
  urldate = {2024-04-02},
  organization = {Eli Bendersky's website}
}

@online{brownleeOrdinalOneHotEncodings2020,
  title = {Ordinal and {{One-Hot Encodings}} for {{Categorical Data}}},
  author = {Brownlee, Jason},
  date = {2020-06-11T19:00:14+00:00},
  url = {https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/},
  urldate = {2024-04-02},
  abstract = {Machine learning models require all input and output variables to be numeric. This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model. The two most popular techniques are an Ordinal Encoding and a One-Hot Encoding. In this tutorial, you will discover how […]},
  langid = {american},
  organization = {Machine Learning Mastery}
}

@online{bushaevStochasticGradientDescent2017,
  title = {Stochastic {{Gradient Descent}} with Momentum},
  author = {Bushaev, Vitaly},
  date = {2017-12-05T14:34:03},
  url = {https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d},
  urldate = {2024-03-25},
  langid = {english},
  organization = {Towards Data Science}
}

@online{chandraMcCullochPittsNeuronMankind2022,
  title = {{{McCulloch-Pitts Neuron}} — {{Mankind}}’s {{First Mathematical Model Of A Biological Neuron}}},
  author = {Chandra, Akshay L.},
  date = {2022-09-27T15:33:34},
  url = {https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1},
  urldate = {2024-03-30},
  abstract = {It is very well known that the most fundamental unit of deep neural networks is called an artificial neuron/perceptron. But the very first…},
  langid = {english},
  organization = {Towards Data Science},
  file = {/Users/rishab/Zotero/storage/S4VH7TG5/mcculloch-pitts-model-5fdf65ac5dd1.html}
}

@misc{fortunerMachineLearningGlossary,
  title = {Machine {{Learning Glossary}}},
  author = {Fortuner, Brendan},
  url = {https://ml-cheatsheet.readthedocs.io/en/latest/index.html},
  urldate = {2024-04-01}
}

@unpublished{garrityMathematicalMaturity2017,
  title = {On {{Mathematical Maturity}}},
  editora = {Garrity, Thomas},
  editoratype = {collaborator},
  date = {2017-07-17},
  url = {https://www.youtube.com/watch?v=zHU1xH6Ogs4},
  urldate = {2024-01-13},
  eventtitle = {27th {{Annual PCMI Summer Session}}},
  venue = {Park City, Utah}
}

@online{haswaniLearningRateDecay2021,
  title = {Learning {{Rate Decay}} and Methods in {{Deep Learning}}},
  author = {Haswani, Vaibhav},
  date = {2021-05-30T15:24:09},
  url = {https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b},
  urldate = {2024-04-03},
  abstract = {While training neural networks with Stochastic or Mini Batch Gradient Descent and a constant learning rate our algorithm usually converges…},
  langid = {english},
  organization = {Analytics Vidhya}
}

@online{kurbielDerivativeSoftmaxFunction2021,
  title = {Derivative of the {{Softmax Function}} and the {{Categorical Cross-Entropy Loss}}},
  author = {Kurbiel, Thomas},
  date = {2021-11-03T07:03:23},
  url = {https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1},
  urldate = {2024-03-29},
  abstract = {A simple and quick derivation},
  langid = {english},
  organization = {Towards Data Science}
}

@article{lecunTheoreticalFrameworkBackpropagation1988,
  title = {A Theoretical Framework for Back-Propagation},
  author = {Lecun, Yann},
  editor = {Touretzky, D. and Hinton, G. and Sejnowski, T.},
  date = {1988},
  journaltitle = {Proceedings of the 1988 Connectionist Models Summer School, CMU, Pittsburg, PA},
  pages = {21--28},
  publisher = {Morgan Kaufmann}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  date = {1943-12},
  journaltitle = {The Bulletin of Mathematical Biophysics},
  shortjournal = {Bulletin of Mathematical Biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  issn = {0007-4985, 1522-9602},
  doi = {10.1007/BF02478259},
  url = {http://link.springer.com/10.1007/BF02478259},
  urldate = {2024-02-23},
  langid = {english}
}

@book{nielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  date = {2015},
  publisher = {Determination Press},
  url = {http://neuralnetworksanddeeplearning.com},
  urldate = {2024-03-29},
  langid = {english}
}

@book{ralstonFutureCollegeMathematics1983,
  title = {The {{Future}} of {{College Mathematics}}},
  editor = {Ralston, Anthony and Young, Gail S.},
  date = {1983},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-5510-9},
  url = {http://link.springer.com/10.1007/978-1-4612-5510-9},
  urldate = {2024-02-23},
  isbn = {978-1-4612-5512-3 978-1-4612-5510-9},
  langid = {english}
}

@report{rosenblatt1957perceptron,
  title = {The Perceptron, a Perceiving and Recognizing Automaton ({{Project PARA}})},
  author = {Rosenblatt, F.},
  date = {1957},
  institution = {Cornell Aeronautical Laboratory},
  location = {Cornell Aeronautical Laboratory},
  url = {https://books.google.de/books?id=P_XGPgAACAAJ}
}

@online{sanderson3Blue1Brown,
  title = {{{3Blue1Brown}}},
  author = {Sanderson, Grant},
  url = {https://www.3blue1brown.com/topics/3blue1brown.com},
  urldate = {2024-03-30},
  abstract = {Mathematics with a distinct visual perspective. Linear algebra, calculus, neural networks, topology, and more.},
  file = {/Users/rishab/Zotero/storage/TXPSLK6Z/neural-networks.html}
}

@online{sharmaActivationFunctionsNeural2022,
  title = {Activation {{Functions}} in {{Neural Networks}}},
  author = {Sharma, Sagar},
  date = {2022-11-20T10:00:50},
  url = {https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6},
  urldate = {2024-04-01},
  abstract = {Sigmoid, tanh, Softmax, ReLU, Leaky ReLU EXPLAINED !!!},
  langid = {english},
  organization = {Towards Data Science}
}

@online{sharmaWhatHellPerceptron2019,
  title = {What the {{Hell}} Is {{Perceptron}}?},
  author = {Sharma, Sagar},
  date = {2019-10-11T05:57:29},
  url = {https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53},
  urldate = {2024-03-31},
  abstract = {The Fundamentals of Neural Networks},
  langid = {english},
  organization = {Towards Data Science}
}

@misc{stanforduniversityCS231nConvolutionalNeural,
  title = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}} – {{Course Notes}}},
  author = {{Stanford University}},
  url = {https://cs231n.github.io/},
  urldate = {2024-03-24},
  abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition. For questions/concerns/bug reports, please submit a pull request directly to our git repo.},
  file = {/Users/rishab/Zotero/storage/DTY4PZ54/cs231n.github.io.html}
}

@book{suzukiArtificialNeuralNetworks2011,
  title = {Artificial {{Neural Networks}} - {{Methodological Advances}} and {{Biomedical Applications}}},
  author = {Suzuki, Kenji},
  date = {2011-04-11},
  abstract = {Artificial neural networks may probably be the single most successful technology in the last two decades which has been widely used in a large variety of applications in various areas. The purpose of this book is to provide recent advances of artificial neural networks in biomedical applications. The book begins with fundamentals of artificial neural networks, which cover an introduction, design, and optimization. Advanced architectures for biomedical applications, which offer improved performance and desirable properties, follow. Parts continue with biological applications such as gene, plant biology, and stem cell, medical applications such as skin diseases, sclerosis, anesthesia, and physiotherapy, and clinical and other applications such as clinical outcome, telecare, and pre-med student failure prediction. Thus, this book will be a fundamental source of recent advances and applications of artificial neural networks in biomedical areas. The target audience includes professors and students in engineering and medical schools, researchers and engineers in biomedical industries, medical doctors, and healthcare professionals.},
  isbn = {978-953-307-243-2},
  file = {/Users/rishab/Zotero/storage/WUYFWVQI/Suzuki - 2011 - Artificial Neural Networks - Methodological Advanc.pdf}
}

@inreference{weissteinHeavisideStepFunction,
  title = {Heaviside {{Step Function}}},
  booktitle = {{{MathWorld--A Wolfram Web Resource}}},
  author = {Weisstein, Eric W.},
  url = {https://mathworld.wolfram.com/HeavisideStepFunction.html},
  urldate = {2024-03-17},
  abstract = {The Heaviside step function is a mathematical function denoted H(x), or sometimes theta(x) or u(x) (Abramowitz and Stegun 1972, p. 1020), and also known as the "unit step function." The term "Heaviside step function" and its symbol can represent either a piecewise constant function or a generalized function. When defined as a piecewise constant function, the Heaviside step function is given by  H(x)=\{0   x{$<$}0; 1/2   x=0; 1   x{$>$}0  (1)   (Abramowitz and Stegun 1972, p....\vphantom\}},
  langid = {english}
}

@inreference{weissteinKroneckerDelta,
  title = {Kronecker {{Delta}}},
  booktitle = {{{MathWorld--A Wolfram Web Resource}}},
  author = {Weisstein, Eric W.},
  publisher = {Wolfram Research, Inc.},
  url = {https://mathworld.wolfram.com/},
  urldate = {2024-03-30},
  abstract = {The simplest interpretation of the Kronecker delta is as the discrete version of the delta function defined by  delta\_(ij)=\{0   for i!=j; 1   for i=j.  (1)   The Kronecker delta is implemented in the Wolfram Language as KroneckerDelta[i, j], as well as in a generalized form KroneckerDelta[i, j, ...] that returns 1 iff all arguments are equal and 0 otherwise. It has the contour integral representation  delta\_(mn)=1/(2pii)∮\_gammaz\^{}(m-n-1)dz,  (2)   where gamma is a contour corresponding...\vphantom\}},
  langid = {english}
}
